## .....

#### 数学建模的六个步骤

<img src="https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230709224359644.png" alt="image-20230709224359644" style="zoom:25%;" />

### 学习资料

[数学建模团队分工协作资料](https://github.com/hhzzzzzr/notes/blob/main/数学建模/引用/数学建模团队分工协作资料（网盘密码一律为1111）.pdf)

[软件安装教程大全最新](https://github.com/hhzzzzzr/notes/blob/main/数学建模/引用/软件安装教程大全最新（网盘密码一律为1111）.pdf)



### 赛题类型

数据预处理赛题、评价类赛题、预测类赛题、优化类赛题

<img src="https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230710152037167.png" alt="image-20230710152037167" style="zoom:60%;" /><img src="https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230710152053348.png" alt="image-20230710152053348" style="zoom:60%;" />



<img src="C:\Users\86178\AppData\Roaming\Typora\typora-user-images\image-20230710152144638.png" alt="image-20230710152144638" style="zoom: 60%;" /><img src="https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230710152208457.png" alt="image-20230710152208457" style="zoom: 60%;" />

### 案例

具体模型案例请看课件等资料



## 一、模型

### 1.1 线性规划

线性规划的目标函数可以是求最大值，也可以是求最小值，约束条件的不等号可以是小于号也可以是大于号。为了避免这种形式多样性带来的不便，$Matlab $中规定线性规划的标准形式
$$
\min_{x}{c^Tx}\\
s.t.
\left\{\begin{matrix} 
 Ax\leq b\\
 Aeq.x=beq\\
 lb\leq x\leq ub
 
 \end{matrix}\right.
$$
其中$ c,x,b,beq,lb,ub$为列向量，$c$称为价值向量，$b$称为资源向量，$A,Aeq$为矩阵。
$Matlab$中求解线性规划的命令为

```matlab
[x,fvall] = linprog(c,A,b)
[x,fvall] = linprog(c,A,b,Aeq,beq)
[x,fvall] = linprog(c,A,b,Aeq,beq,lb,ub)
```


其中$x$返回的是决策的取值，$fval $的数的优值$c$为价值量，$A，b $对应的是线性不等式约束，$Aeq，beq$对应的是线性等式约束，$lb,ub$分别对应的是决策量的下界向量和上界量。



### 1.2 整数规划

整数规划最优解不能按照实数最优解简单取整而获得。

整数规划的数学模型
$$
max(min)z=\sum_{j=1}^{n}c_jx_j
\\
s.t.
\left\{\begin{array}{}
\sum_{j=1}^{n}{a_{ij}}x_j\leq(=,\ge )b_i\ \ (i=1,2,...,m)
\\x_j\ge0,x_j为整数(j=1,2,...,n)
\end{array}
\right.
$$
依照决策变量取整要求的不同，整数规划可分为纯整数规划、全整数规划、混合整数规划、0－1整数规划

目前，常用的求解整数规划的方法有：分枝定界法和割平面法；对于特别的0－1规划问题采用隐枚举法和匈牙利法

#### 分支定界法

不考虑整数限制先求出相应松弛问题的最优解，若松弛问题无可行解，则$ILP$无可行解;若求得的松弛问题最优解符合整数要求，则是$ILP$的最优解

若不满足整数条件，则任选一个不满足整数条件的变量$x_i^0$来构造新的约束添加到松弛问题中形成两个子问题$x_i\le[x_i^0];x_i\ge[x_i^0]+1$依次在缩小的可行域中求解新构造的线性规划的最优解，并重复上述过程，直到子问题无解或有整数最优解(被查清)

#### 割平面法

如果松弛问题$(P_0)$无解，则$(P)$无解；如果$(P_0)$的最优解为整数向量，则也是$(P)$的最优解；

如果$(P_0)$的解含有非整数分量，则对(P增加平面条件:即对$(P_0)$增加一个线性约束，将$(P_0)$的可行区域割掉一块，使得非整数解恰好在割掉的一块中，但又没有割掉原问题$(P)$的可行解，得到问题$(P)$，重复上述的过程。



#### 0-1变量的使用

##### 互斥约束问题

从下述$p$个约束条件中恰好选择$q$个约束条件$\sum_{j=1}^na_{ij}x_j\le b_i(i=1,2,...,p)$且
$$
y_i=\left\{\begin{array}{}
0 & 选第i个约选第i个约束条件束条件
\\1&选第i个约束条件
\end{array}
\right.
(i=1,...,p)
$$
则
$$
\left\{\begin{array}{}
\sum_{j=1}^na_{ij}x_j\le b_i+My_i
\\\sum_{i=1}^py_i=p-q
\end{array}
\right.
(i=1,2,...,p)
$$
其中$M$为一个极大的数





#### 指派问题的数学模型

##### 标准形式

$$
\begin{array}{l}
x_{i j}=\left\{\begin{array}{ll}
1 & \text { 第i个人做第 } j \text { 硕工作 } \\
0 & \text { 第 } i \text { 个人不做第 } j \text { 项工作 }
\end{array}(i, j=1, \cdots, n)\right. \\
\min z=\sum_{i=1}^{n} \sum_{j=1}^{n} c_{i j} x_{i j}\\
\text {代价系数矩阵:}
X=\left(x_{i j}\right)_{n \times n}=\left(\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n 1} & x_{n 2} & \cdots & x_{n n}
\end{array}\right) \\
 \\s.t.  \left\{\begin{array}{c}\sum_{j=1}^{n} x_{i j}=1(i=1, \cdots, n) \\ \sum_{i=1}^{n} x_{i j}=1(j=1, \cdots, n) \\ x_{i j}=0 \text { 或 } 1(i, j=1, \cdots, n)\end{array}\right. 

\end{array}
$$

##### 在非标准形式中

1. 最大化指派问题
   $ C=\left(c_{i j}\right)_{n \times n}  中最大元素为  m , 令  B=\left(b_{i j}\right)_{n \times n}=\left(m-c_{i j}\right)_{n \times n} $
2. 人数和工作数不等
   人少工作多: 添加虚拟的“人”，代价都为 0
   人多工作少: 添加虚拟的工作, 代价都为 0
3. 一个人可做多件工作
   该人可化为几个相同的 “人”
4. 某工作一定不能由某人做
   该人做该工作的相应代价取足够大M

##### 指派问题的匈牙利解法的一般步骤

自行查阅



### 1.3 非线性规划

一般形式：
$$
\min f(x)\\
s.t.\left\{\begin{array}{}h_j(x)\le 0 & j=1,2,...,q\\
g_i(x)=0& i=1,2,...,p
\end{array}
\right.
$$
在一组等式或不等式的约束下，求一个函数的最大值（或最小值）问题，其中至少有一个非线性函数，这类问题称之为非线性规划问题



$Matlab$中写为以下形式
$$
\min f(x)
\\s.t.\left\{\begin{array}{}
A\cdot x\le b
\\Aeq\cdot x=beq
\\c(x)\le 0
\\ceq(x)=0
\\lb\le x\le ub 
\end{array}\right.
$$


命令写为：

```matlab
[x,fval]=fmincon(fun,x0,A,b,Aeq,beq,lb,ub,nonlcon,options)
```

##### 二次规划

$Matlab$中求解二次规划的命令是

```matlab
[x,fval] = quadprog(H,f,A,b,Aeq,beq,lb,ub,x0,options)
```



### 1.4 层次分析法

**步骤：**

#### （1）建立层次结构模型

该结构图包括目标层，准则层，方案层

#### （2）构造成对比较矩阵

从第二层开始用成对比较矩阵和1-9尺度

#### （3）计算单排序权向量并做一致性检验

对每个成对比较矩阵计算最大特征值及其对应的特征向量，利用一致性指标$(CI)$、随机一致性指标$(RI)$和一致性比率$(CR)$做一致性检验，当$CR<0.1$时，认为$A$的不一致程度在容许范围内。若检验通过，特征向量(归一化后)即为权向量;若不通过需要重新构造成对比较矩阵。

其中：
$$
CI=\frac{\lambda-n}{n-1}
\\CR=\frac{CI}{RI}
$$
![image-20230711222637335](https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230711222637335.png)

#### （4）计算总排序权向量并做一致性检验

计算最下层对最上层总排序的权向量，设$B_j$层为下层元素，$A_i$层为上层元素。则$B$层的第$i$个元素对总目标权值为:
$$
\sum_{j=1}^ma_jb_{ij}
$$
利用总排序一致性比率（当$CR<0.1$时，认为层次总排序通过一致性检验）
$$
AR=\frac{a_1CI_1+a_2CI_2+...+a_mCI_m}{a_1RI_1+a_2RI_2+...+a_mRI_m}
$$


进行检验。若通过，则可按照总排序权向量表示的结果进行决策，否则需要重新考虑模型或重新构造那些一致性比率CR较大的成对比较矩阵。



### 1.5 TOPSIS模型

**步骤：**

#### （1）用向量规划化的方法求得规范决策阵

设多属性决策问题的决策矩阵$A=(a_{ij})_{m\times n}$，规范化决策矩阵$B=(b_{ij})_{m\times n}$，其中
$$
b_{ij}=\frac{a_{ij}}{\sqrt{\sum_{i=1}^ma_{ij}^2}},i=1,2,...,m ,j=1,2,...,n
$$
（规范化的方法有多种，如线性变换、标准$0-1$变换、区间型变换、向量规范化、标准化处理等，上面的方法为向量规范化）

#### （2）构成加权规范矩阵$C=(c_{ij})_{m\times n}$

设由决策人给定各属性的权重向量为$w={[w_1,w_2,...,w_n]}^T$，则
$$
c_{ij}=w_j\cdot b_{ij},i=1,2,...,m ,j=1,2,...,n
$$

#### （3）确定正理想解$C^*$和负理想解$C^0$

设正理想解$C^*$的第$j$个属性值为$c_j^*$，负理想解$C^0$第$j$个属性值为$c_j^0$，则

正理想解：
$$
c_j^*=\left\{\begin{array}{}
\max_{i}c_{ij},&j为效益型属性，\\
\min_ic_{ij},&j为成本型属性，
\end{array}
\right.
j=1,2,...,n
$$
负理想解：
$$
c_j^*=\left\{\begin{array}{}
\min_{i}c_{ij},&j为效益型属性，\\
\max_ic_{ij},&j为成本型属性，
\end{array}
\right.
j=1,2,...,n
$$

#### （4）计算各方案到正理想解与负理想解的距离

备选方案$d_i$到正理想解的距离
$$
s_i^*=\sqrt{\sum_{j=1}^{n}(c_{ij}-c_j^*)^2},i=1,2,...,m.
$$


备选方案$d_i$到负理想解的距离
$$
s_i^0=\sqrt{\sum_{j=1}^{n}(c_{ij}-c_j^0)^2},i=1,2,...,m.
$$

#### （5）计算各方案的排队指标值（即综合评价指数）

$$
f_i^*=\frac{s_i^0}{s_i^0+s_i^*},i=1,2,...,m
$$

#### （6）按$f_i^*$由大到小排列方案的优劣次序



### 1.6 聚类分析

聚类分析有两种：一种是对样品的分类，称为Q型，另一种是对变量（指标）的分类，称为R型。

#### 样品间的相似度量—距离

设有$  \mathrm{n}  $个样品的$  \mathrm{p}  $元观测数据：
$$
x_{i}=\left(x_{i 1}, x_{i 2}, \cdots, x_{i p}\right)^{T}, i=1,2, \cdots, n
$$
这时, 每个样品可看成$  \mathrm{p}  $元空间的一个点, 每两个点 之间的距离记为$  d\left(x_{i}, x_{j}\right)  $满足条件:
$$
\begin{array}{l}
d\left(x_{i}, x_{j}\right) \geq 0 \\
d\left(x_{i}, x_{j}\right)=0 \text { 当且仅当 } x_{i}=x_{j} \\
d\left(x_{i}, x_{j}\right)=d\left(x_{j}, x_{i}\right) \\
d\left(x_{i}, x_{j}\right) \leq d\left(x_{i}, x_{k}\right)+d\left(x_{k}, x_{j}\right)
\end{array}
$$

1. 欧氏距离：$  d\left(x_{i}, x_{j}\right)=\left[\sum_{k=1}^{p}\left(x_{i k}-x_{j k}\right)^{2}\right]^{1 / 2} \quad $		`pdist(x) `

2. 绝对距离：$  d\left(x_{i}, x_{j}\right)=\sum_{k=1}^{p}\left|x_{i k}-x_{j k}\right| \quad$                   `pdist(x , 'cityblock')`

3. 明氏距离：$   d\left(x_{i}, x_{j}\right)=\left[\sum_{k=1}^{p}\left|x_{k k}-x_{j k}\right|^{m}\right]^{l / m} $          `pdist(x,'minkowski',r)`

4. 切氏距离：$  d\left(x_{i}, x_{j}\right)=\max _{\substack{l k s p}}\left|x_{k k}-x_{j k}\right| \quad $              `max(abs(xi-xj))`

5. 方差加权距离：$  d\left(x_{i}, x_{j}\right)=\left[\sum_{k=1}^{p}\left(x_{i k}-x_{j k}\right)^{2} / s_{k}^{2}\right]^{1 / 2} $

  

  将原数据标准化以后的欧氏距离

6. 马氏距离：$  d\left(x_{i}, x_{j}\right)=\sqrt{\left(x_{i}-x_{j}\right)^{T} \Sigma^{-1}\left(x_{i}-x_{j}\right)} \quad$ `pdist(x,'mahal') `

7. 兰氏距离：$  d\left(x_{i}, x_{j}\right)=\frac{1}{p} \sum_{k=1}^{p} \frac{\left|x_{i k}-x_{j k}\right|}{x_{i k}+x_{j k}} $

8. 杰氏距离 (Jffreys \& Matusita)：$d\left(x_{i}, x_{j}\right)=\left[\sum_{k=1}^{p}\left(\sqrt{x_{i k}}-\sqrt{x_{j k}}\right)^{2}\right]^{1 / 2}$

#### 变量间的相似度量——相似系数

当对$p$个指标变量进行聚类时，用相似系数来衡量变量之间的相似程度（关联度），若用$C_{\alpha\beta}$表示变量之间的相似系数，则应满足
$$
|C_{\alpha\beta}|\le1，且C_{\alpha\alpha}=1\\
C_{\alpha\beta}=\pm1，当且仅当\alpha=k\beta,k\neq0\\
c_{\alpha\beta}=C_{\beta\alpha}
$$
相似系数中最常用的是相关系数与夹角余弦。

##### 变量间的夹角余弦

$$
C_{ij}(1)=\cos{\alpha_{ij}}=\frac{\sum_{t=1}^nx_{ti}x_{tj}}{\sqrt{\sum_{t=1}^nx_{ti}^2}\sqrt{\sum_{t=1}^nx_{tj}^2}}
$$

##### 相关系数

$$
c_{ij}(2)=\frac{\sum_{t=1}^n(x_{ti}-\bar x_i)(x_{tj}-\bar x_j)}{\sqrt{\sum_{t=1}^n(x_{ti}-\bar x_i)^2}\sqrt{\sum_{t=1}^n(x_{tj}-\bar x_j)^2}}
$$

#### 类间距离

设$d_{ij}$为样品$x_j,x_i$间的距离，$G_p,G_q$分别表示两个类别，各含有$n_p,n_q$个样品

（1）最短距离
$$
D_{pq}=\min_{i\in G_p,j\in G_q}d_{ij}
$$
（2）最长距离
$$
D_{pq}=\max_{i\in G_p,j\in G_q}d_{ij}
$$
（3）类平均距离
$$
D_{pq}=\frac{1}{n_pn_q}\sum_{i\in G_p}\sum_{j\in G_q}d_{ij}
$$
（4）重心距离
$$
D_{pq}=d(\bar x_p,\bar x_q)=\sqrt{(\bar x_p-\bar x_q)^T(\bar x_p-\bar x_q)}
$$
（5）离差平方和距离
$$
D_{pq}^2=\frac{n_pn_q}{n_p-n_Q}(\bar x_p-\bar x_q)^T(\bar x_p-\bar x_q)
$$

#### 谱系聚类法的步骤

1. 选择样本间距离的定义及类间距离的定义
2. 计算n个样本两两之间的距离，得到距离矩阵$D=(d_{ij})$
3. 构造个类，每类只含有一个样本
4. 合并符合类间距离定义要求的两类为一个新类
5. 计算新类与当前各类的距离。若类的个数为1，则转到步骤6，否则回到步骤4;
6. 画出聚类图;
7. 决定类的个数和类

#### K—平均聚类算法(K_means)

基本思想：

1. 首先，随机的选择k个对象，每个对象初始的代表了一个簇的平均值
2. 对剩余的每个对象，根据其与各个簇中心的距离将它赋给最近的簇
3. 然后重新计算每个簇的平均值
4. 这个过程不断重复，直到准则函数收敛

通常选择均方差作为收敛准则函数
$$
E=\sum_{i=1}^k\sum_{p\in C_i}|p-m_i|^2
$$
其中$ E$为数据库中所有对象的均方差之和;$p$为代表对象的空间中的一个点;$m_i$为聚类$C_i$的均值($p$和$m_i$均是多维的)

这个准则试图使得生成的结果尽可能地紧凑和独立:当结果簇是密集的，且簇与簇之间区别明显时，算法的效果较好。

### 1.7 灰色关联系分析与预测模型

#### 灰色关联度

选取参考数列
$$
X_0=\{X_0(k)|k=1,2,...,n\}=(X_0(1),X_0(2),...,X_0(n))
$$
其中$k$表示时刻

假设有$m$个比较数列
$$
X_i=\{X_ii(k)|k=1,2,...,n\}=(X_i(1),X_i(2),...,Xi(n))(i=1,2,...,m)
$$
则称
$$
\zeta_{i}(k)=\frac{\min _{i} \min _{k}\left|X_{0}(k)-X_{i}(k)\right|+\rho \max _{i} \max _{k}\left|X_{0}(k)-X_{i}(k)\right|}{\left|X_{0}(k)-X_{i}(k)\right|+\rho \max _{i} \max _{k}\left|X_{0}(k)-X_{i}(k)\right|}
$$
为比较数列$  X_{i}  $对参考数列$  X_{0}  $在$  k  $时刻的关联系数, 其中$  \rho \in[0,+\infty)  为$分辨系数。一般来讲, 分辨系数$  \rho \in[0,1] $, 由 (1) 容易看出, $ \rho  $越大, 分辩率越大;  $\rho  $越小, 分辩率越小。
式 (31) 定义的关联系数是描述比较数列与参考数列在某时刻关联程度的一种指标, 由于各个时刻都有一个关联系数, 因此信息显得过于分散, 不便于比较, 为此我们给出
$$
r_{i}=\frac{1}{n} \sum_{k=1}^{n} \zeta_{i}(k)
$$
为比较数列$  X_{i}  $对参考数列$  X_{0}  $的关联度。

应该指出的是, 公式 (31) 中的$  \left|X_{0}(k)-X_{i}(k)\right|  $不能 区别因素关联是正关联还是负关联, 可采取下面办法解决这个问题。记


$$
\begin{aligned}
\sigma_{i} & =\sum_{k=1}^{n} k X_{i}(k)-\sum_{k=1}^{n} X_{i}(k) \sum_{k=1}^{n} \frac{k}{n} \\
\sigma_{n} & =\sum_{k=1}^{n} k^{2}-\left(\sum_{k=1}^{n} k\right) 2 / n
\end{aligned}
$$
则当$  \operatorname{sign}\left(\frac{\sigma_{i}}{\sigma_{n}}\right)=\operatorname{sign}\left(\frac{\sigma_{j}}{\sigma_{n}}\right) $，则$  X_{i}  $和$  X_{j}  $为正关联;

当$ \operatorname{sign}\left(\frac{\sigma_{i}}{\sigma_{n}}\right)=-\operatorname{sign}\left(\frac{\sigma_{j}}{\sigma_{n}}\right) $，则$ X_{i} $和$X_{j}$为负关联。

#### 灰色生成数列

一切灰色序列都能通过某种生成弱化其随机性，显现其规律性。数据生成的常用方式有累加生成、累减生成和加权累加生成。

##### （1）累加生成

把数列各项(时刻) 数据依次累加的过程称为累加生成过程由累加生成过程所得的数列称为累加生成数列。设原始数列为
$$
x^{(0)}=(x^{(0)}(1),x^{(0)}(2),...,x^{(0)}(n)
\\x^{(1)}(k)=\sum^{k}_{i=1}x^{0}(i),k=1,2,...,n
$$
令
$$
x^{(1)}=(x^{(1)}(1),x^{(1)}(2),...,x^{(1)}(n))
$$
称所得到的新数列为数列$x^{(0)}$的1次累加生成数列。类似地有
$$
x^{(r)}(k)=\sum^{k}_{i=1}x^{(r-1)}(i),k=1,2,...,n,r\ge1
$$
称为数列$x^{(0)}$的$r$次累加生成数列

##### （2）累减生成

对于原始数据列依次做前后相邻的两个数据相减的运算过程称为累减生成过程$IAGO$。如果原始数据列为
$$
x^{(1)}=\left(x^{(1)}(1), x^{(1)}(2), \cdots, x^{(1)}(n)\right)
$$
 令
$$
  x^{(0)}(k)=x^{(1)}(k)-x^{(1)}(k-1), k=2,3, \cdots, n 
$$
称所得到的数列$  x^{(0)}  $为$  x^{(1)}  $的1次累减生成数列。 注: 从这里的记号也可以看到, 从原始数列$  x^{(0)}$ , 得 到新数列$  x^{(1)}$ , 再通过累减生成可以还原出原始数列。 实际运用中在数列$  x^{(1)}  $的基础上预测出$  \hat{x}^{(1)} $, 通过 累减生成得到预测数列$  \hat{x}^{(0)}  $。

##### （3）加权邻值生成

设原始数列为
$$
x^{(0)}=\left(x^{(0)}(1), x^{(0)}(2), \cdots, x^{(0)}(n)\right)
$$
称$  x^{(0)}(k-1), x^{(0)}(k)  $为数列$  x^{(0)}  $的任意一对邻值。$  x^{(0)}(k-1)  $为后邻值,$  x^{(0)}(k)  $为前邻值, 对于常 数$  \alpha \in[0,1] $, 令$z^{(0)}(k)=\alpha x^{(0)}(k)+(1-\alpha) x^{(0)}(k-1), k=2,3, \cdots,n,$由此得到的数列$  z^{(0)}  $称为数列$  x^{(0)}  $在权$  \alpha  $下的邻值生 成数, 权$  \alpha  $也称为生成系数。
特别地, 当生成系数$  \alpha=0.5  $时, 则称
$$
z^{(0)}(k)=0.5 x^{(0)}(k)+0.5 x^{(0)}(k-1), k=2,3, \cdots, n,
$$
为均值生成数, 也称等权邻值生成数。

#### 灰色模型GM(1,1)

灰色模型是利用离随机数经过生成变为随机性被显著削弱而且较有规律的生成数，建立起的微分方程形式的模型，这样便于对其变化过程进行研究和描述

预测步骤：

##### （1）数据的检验预处理

为了保证$GM  (1,1)  $建模方法的可行性, 需要对已知数据做必要的检验处理。
设原始数据列为$ x^{(0)}=\left(x^{(0)}(1), x^{(0)}(2), \cdots, x^{(0)}(n)\right) $, 计算数列的级比
$$
\lambda(k)=\frac{x^{(0)}(k-1)}{x^{(0)}(k)}, k=2,3, \cdots, n
$$
如果所有的级比都落在可容覆盖区间$  X=\left(e^{\frac{-2}{n+1}}, e^{\frac{2}{n+1}}\right) $内, 则数据列$  x^{(0)}  $可以建$立  \mathrm{GM}(1,1)  $模型且可以进行灰色预测。
否则对数据做适当的变换处理, 如平移变换:取$c$使得数据列
$$
y^{(0)}(k)=x^{(0)}(k)+c, k=1,2, \cdots, n,
$$
的级比都落在可容覆盖内。

##### （2）建立GM（1, 1) 模型

不妨设$  x^{(0)}=\left(x^{(0)}(1), x^{(0)}(2), \cdots, x^{(0)}(n)\right) $满足上面的要求, 以它为数据列建立$  G M(1,1)  $模型
$$
x^{(0)}(k)+a z^{(1)}(k)=b,
$$
用回归分析求得$  a, b  $的估计值, 于是相应的白化模型为
$$
\frac{d x^{(1)}(t)}{d t}+a x^{(1)}(t)=b
$$
解为
$$
x^{(1)}(t)=\left(x^{(0)}(1)-\frac{b}{a}\right) e^{-a(t-1)}+\frac{b}{a} .
$$
于是得到预测值
$$
\hat{x}^{(1)}(k+1)=\left(x^{(0)}(1)-\frac{b}{a}\right) e^{-a k}+\frac{b}{a}, k=1,2, \cdots, n-1,
$$
从而相应地得到预测值:
$$
\hat{x}^{(0)}(k+1)=\hat{x}^{(1)}(k+1)-\hat{x}^{(1)}(k), k=1,2, \cdots, n-1,
$$

##### （3）检验预测值

+ 残差检验: 计算相对残差

$$
\varepsilon(k)=\frac{x^{(0)}(k)-\hat{x}^{(0)}(k)}{x^{(0)}(k)}, k=1,2, \cdots, n,
$$

如果对所有的$  |\varepsilon(k)|<0.1 $, 则认为达到较高的要求: 否则, 若对所有的$  |\varepsilon(k)|<0.2 $, 则认为达到一般要求。

+ 级比偏差值检验: 计算

$$
\rho(k)=1-\frac{1-0.5 a}{1+0.5 a} \lambda(k),
$$

如果对所有的$  |\rho(k)|<0.1 $, 则认为达到较高的要求; 否则若对所有的$  |\rho(k)|<0.2 $, 则认为达到一般要求。

### 1.8 数据预处理

数据预处理的任务：

+ 数据清理（清洗）

数据清洗主要是删除原始数据集中的无关数据、重复数据，平滑噪声数据，处理缺失值、异常值等

+ 数据集成

将多个数据源合并成一致的数据存储，构成一个完整的数据集，如数据仓库。

+ 数据归约（消减）

通过聚集、删除冗余属性或聚类等方法来压缩数据。

+ 数据变换（转换）

将一种格式的数据转换为另一格式的数据(如规范化)。如建党函数变换、归一化（最大-最小规范化、零-均值规范化、小数定标规范化）

#### 1.8.1 缺失值处理

处理缺失值的方法可分为三类:删除记录、数据插补和不处理。

常用的数据插补方法：

##### 1.8.1.1 均值/中位数/众数插 

根据属性值的类型，用该属性取值的平均数/中位数/众数进行插补

##### 1.8.1.2 使用固定值

将缺失的属性值用一个常量替换。如广州一个工厂普通外来务工人员的“基本工资”属性的空缺值可以用 2015 年广州市普通外来务工人员工资标准 1895 元/月，该方法就是使用固定值。

##### 1.8.1.3 最近临插补

在记录中找到与缺失样本最接近的样本的该属性值插补

欧几里得距离 (Euclidean Distance ) : 又称直线距离。
·令$i =(x_{i1},x_{i2},...,x_{ip})和j = (x_{j1},x_{j2},...,x_{jp})$是两个被$p$个数值属性描述的对象。对象$i$和$j$之间的欧几里得距离为:
$$
d(i,j)=\sqrt{(x_{i1}-x_{j1})^2+(x_{i1}-x_{j2})^2)+...+(x_{ip}-x_{jp})^2)}
$$

##### 1.8.1.4 回归方法

(no)

多元线性回归方程为:$\hat{Y}=\hat{a}+\hat{b}_{1} X_{1}+\hat{b}_{2} X_{2}+\ldots+\hat{b}_{k} X_{k} $
其中，$  \hat{a}  $为回归方程的常数项，$  \hat{b}_{1}, \hat{b}_{2}, \ldots, \hat{b}_{k}  $为偏回归系数。多项式回归的一般形式为:
$$
\hat{y}_{i}=\hat{a}+\hat{b}_{1} x_{1}+\hat{b}_{2} x_{2}+\ldots+\hat{b}_{k} x_{k}
$$
对于$n$组样本$  x_{1 i}, x_{2 i}, x_{3 i}, \ldots, y_{i}(\mathrm{i}=1,2, \ldots, n)  ，$其回归方程组形式为:
$$
\hat{y}_{i}=\hat{a}+\hat{b}_{1} x_{1 i}+\hat{b}_{2} x_{2 i}+\ldots+\hat{b}_{k} x_{k i}

$$
即:
$$
\left\{\begin{array}{l}
\hat{y}_{1}=\hat{a}+\hat{b}_{1} x_{11}+\hat{b}_{2} x_{21}+\ldots+\hat{b}_{k} x_{k 1} \\
\hat{y}_{2}=\hat{a}+\hat{b}_{1} x_{12}+\hat{b}_{2} x_{22}+\ldots+\hat{b}_{k} x_{k 2} \\
\ldots \ldots \\
\hat{y}_{n}=\hat{a}+\hat{b}_{1} x_{1 n}+\hat{b}_{2} x_{2 n}+\ldots+\hat{b}_{k} x_{k n}
\end{array}\right.
$$

##### 1.8.1.5 插值法

###### 拉格朗日揷值法

$$
f(x)=\sum_{i=0}^{n} y_{i} \frac{\left(x-x_{0}\right) \cdots\left(x-x_{i-1}\right)\left(x-x_{i+1}\right) \cdots\left(x-x_{n}\right)}{\left(x_{i}-x_{0}\right) \cdots\left(x_{i}-x_{i-1}\right)\left(x_{i}-x_{i+1}\right) \cdots\left(x_{i}-x_{n}\right)}
$$

第一步 :
求已知的$  \mathrm{n}  $个点对$  \left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right) \cdots\left(x_{n}, y_{n}\right)  $的基函数
$$
l_{i}\left(x_{j}\right)=\prod_{j=o, j \neq i}^{n} \frac{x-x_{j}}{x_{i}-x_{j}}
$$
第二步 :
求已知的$  \mathrm{n}  $个点对$  \left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right) \cdots\left(x_{n}, y_{n}\right)  $的揷值多项式
$$
L(x)=\sum_{i=0}^{n} y_{i} \prod_{j=0, j \neq i}^{n} \frac{x-x_{j}}{x_{i}-x_{j}}
$$
第三步 :

将缺失的函数值对应的点代入插值多项式得到缺失值的近似值$ L(x)$

###### 牛顿揷值法 

$$
P_{n}(x)=a_{0}+a_{1}\left(x-x_{0}\right)+a_{2}\left(x-x_{0}\right)\left(x-x_{1}\right)+\ldots+a_{n}\left(x-x_{0}\right) . .\left(x-x_{n-1}\right)
$$

第一步:
求已知的$  \mathrm{n}  $个点对$  \left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{n}, y_{n}\right.  )$的所有阶差商公式
$$
\begin{array}{c}
f\left[x_{1}, x\right]=\frac{f[x]-f\left[x_{1}\right]}{x-x_{1}}=\frac{f(x)-f\left(x_{1}\right)}{x-x_{1}} ; \quad f\left[x_{2}, x_{1}, x\right]=\frac{f\left[x_{1}, x\right]-f\left[x_{2}, x_{1}\right]}{x-x_{2}} ; \\
f\left[x_{3}, x_{2}, x_{1}, x\right]=\frac{f\left[x_{2}, x_{1}, x\right]-f\left[x_{3}, x_{2}, x_{1}\right]}{x-x_{3}}\\...
\end{array}
$$
第二步 :
联立以上差商公式建立如下揷值多项式$  f(x) $
$$
\begin{aligned}
f(x)= & f\left(x_{1}\right)+\left(x-x_{1}\right) f\left[x_{2}, x_{1}\right]+\left(x-x_{1}\right)\left(x-x_{2}\right) f\left[x_{3}, x_{2}, x_{1}\right]+ \\
& \left(x-x_{1}\right)\left(x-x_{2}\right)\left(x-x_{3}\right) f\left[x_{4}, x_{3}, x_{2}, x_{1}\right]+\cdots+ \\
& \left(x-x_{1}\right)\left(x-x_{2}\right) \cdots\left(x-x_{n-1}\right) f\left[x_{n}, x_{n-1}, \cdots, x_{2}, x_{1}\right]+ \\
& \left(x-x_{1}\right)\left(x-x_{2}\right) \cdots\left(x-x_{n}\right) f\left[x_{n}, x_{n-1}, \cdots, x_{1}, x\right]
\end{aligned}
$$
第三步：

将缺失的函数值对应的点 代入插值多项式得到缺失值的近似值$ f(x)$

#### 1.8.2 异常值的检测

![image-20230714225327000](https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230714225327000.png)

##### 1.8.2.1 画象限图法

![image-20230714230342868](https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230714230342868.png)

##### 1.8.2.2 

计算$n$维数据集中所有样本间的测量距离，如果样本$S$中至少有一部分数量为$p$的样本到$S_i$的距离比$d$大，那么样本$S_i$是数据集$S$中的一个噪声数据

#### 1.8.3 特征工程

特征工程的处理流程为首先去掉**无用特征**，接着去除**冗余的特征**，如共线特征，并利用存在的特征、转换特征、内容中的特征以及其他数据源**生成新特征**，然后对**特征进行转换**（数值化、类别转换、归一化等)，最后对**特征进行处理**（异常值、最大值、最小值，缺失值等)， 以符合模型的使用。 简单来说，特征工程的处理一般包括**数据预处理、特征处理、特征选择**等工作，而特征选择视情况而定，如果特征数量较多，则可以进行特征选择等操作。

##### 特征选择

特征选择的方法有过滤法、包装法、嵌入法。

常用过滤法，即按照发散性或者相关性对各个特征进行评分，通过设定阈值或者待选择阈的个数来选择特征。

###### 主成分析法

主成分分析是一种通过**降维**技术把多个变量化为少数几个主成分(即**综合变量**)的多元统计方法，这些主成分能够反映原始变量的大部分信息，通过表示为原始变量的线性组合，为了使得这些主成分所包含的信息互不重叠，要求各主成分之间互不相关。

### 1.9 插值与拟合

#### 1.9.1 插值

常用的插值方法有$Lagrange$插值法和$Newton$插值法。

##### 1.9.1 拉格朗日插值法

$$
f(x)=\sum_{i=0}^{n} y_{i} \prod_{j=0, j \neq i}^{n} \frac{x-x_{j}}{x_{i}-x_{j}}
$$

##### 1.9.2 高次插值的Runge现象

插值多项式的次数越高，插值精度越高。但仅仅在插值多项式的次数不超过七时成立; 插值多项式的次数超过七时，插值多项式会出现严重的振荡现象，称之为$Runge$现象。因此，在实际中不应使用七次以上的插值。

避免$Runge$现象的常用方法是: 将插值区间分成若干小区间，在小区间内用低次(二次，三次)插值，即分段低次插值，如样条函数插值。

##### 1.9.3 MATLAB插值

```matlab
yi= interp1(x,y,xi,'method');	%一维插值
zi= interp1(x,y,z,xi,yi,'method');	%一维插值
```



#### 1.9.2 拟合

拟合问题与插值问题的区别在于:
(1) 插值函数过已知点，而拟合函数不定过已知点;

(2) 插值主要用于求函数值，而拟合的主要目的是求函数关系，从而进行预测等进一步的分析。当然，某些特定问题既可以用插值也可以用拟合。

##### 拟合计算

曲线拟合需解决如下两个问题:
(1) 线型的选择;
(2) 线型中参数的计算。

线型的选择是拟合计算的关键和难点。通常主要根据专业知识和散点图确定线型

线性拟合中参数的计算可采用最小二乘法，而非线性拟合参数的计算则要应用Gauss-Newton迭代法。

##### Matlab拟合

```matlab
[a,S]=polyfit(x,y,n);		%n为拟合多项式的次数；a为拟合多项式系数所构成的向量；S为分析拟合效果所需要的指标
[b,r]=polyfit(x,y,fun,b0,option];%非线性拟合；fun为拟合函数；b0为拟合次数的初始迭代值；option为拟合选项；b为拟合参数；r为拟合残差
a=nlinfit(x,y,fun,bo);		%非线性拟合

```

### 1.10 回归分析

![image-20230717221901162](https://gitee.com/passing-the-world-with-you/typora_pictures/raw/master/img/image-20230717221901162.png)

#### 1.10.1 一元线性回归

##### 1.10.1.1 数学模型

般地，称由$y=\beta_0+ \beta_1x+ \varepsilon $确定的模型为**一元线性回归模型**记为
$$
\left\{\begin{array}{}
y=\beta_0+ \beta_1x+ \varepsilon \\
E\varepsilon =0,D\varepsilon=\sigma^{2}
\end{array}
\right.
$$
固定的末知参数$  \beta_{0} 、 \beta_{1}  $称为回归系数, 自变量$ x  $也称为回归变量.

$Y=\beta_{0}+\beta_{1} x$称为$y$对$x$的回归直线方程. 

一元线性回归分析的主要任务是:
1. 用试验值（样本值）对$  \beta_{0} 、 \beta_{1}  $和$  \sigma  $作点估计;
2. 对回归系数$  \beta_{0} 、 \beta_{1}  $作假设检验;
3. 在$  x=x_{0}  $处对$  y  $作预测, 对$  y  $作区间估计.

##### 1.10.1.2 普通最小二乘法(OLS)

给定一组样本观测值$  \left(X_{i}, Y_{i}\right), i=1,2, \ldots \mathrm{n} $, 假如模型参数估计量已经求得, 并且是最合理的参数估计量, 那么样本回归函数应该能够最好地拟合样本数据, 即样本回归线上的点与真实观测点的 “总体误差” 应该尽可能地小。

普通最小二乘法给出的判断标准是：二者之差的平方和最小, 即
$$
Q=\sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(Y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1} X_{i}\right)\right)^{2} \rightarrow \text { 最小 }
$$
由于$  Q=\sum_{1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}=\sum_{1}^{n}\left(Y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1} X_{i}\right)\right)^{2}  $是$  \hat{\beta}_{0} 、 \hat{\beta}_{1}  $的二次函数, 并且非负, 所以其极小值总是存在的。
根据极值存在的条件, 当$  Q $ 对  \hat${\beta}_{0} 、 \hat{\beta}_{1}  $的一阶偏导数为 0 时, $ Q  $达到最小。即
$$
\left\{\begin{array} { l } 
{ \frac { \partial Q } { \partial \hat { \beta } _ { 0 } } = 0 } \\
{ \frac { \partial Q } { \partial \hat { \beta } _ { 1 } } = 0 }
\end{array} \Rightarrow \left\{\begin{array} { c } 
{ \sum ( Y _ { i } - \hat { \beta } _ { 0 } - \hat { \beta } _ { 1 } X _ { i } ) = 0 } \\
{ \sum ( Y _ { i } - \hat { \beta } _ { 0 } - \hat { \beta } _ { 1 } X _ { i } ) X _ { i } = 0 }
\end{array} \Rightarrow \left\{\begin{array}{l}
\Sigma Y_{i}=n \hat{\beta}_{0}+\hat{\beta}_{1} \Sigma X_{i} \\
\Sigma Y_{i} X_{i}=\hat{\beta}_{0} \Sigma X_{i}+\hat{\beta}_{1} \Sigma X_{i}^{2}
\end{array}\right.\right.\right.
$$
解得:
$$
\left\{\begin{array}{l}
\hat{\beta}_{1}=\frac{n \Sigma Y_{i} X_{i}-\Sigma Y_{i} \Sigma X_{i}}{n \Sigma X_{i}^{2}-\left(\Sigma X_{i}\right)^{2}} \\
\hat{\beta}_{0}=\bar{Y}-\hat{\beta}_{1} \bar{X}
\end{array}\right.
$$
由于$  \hat{\beta}_{0} 、 \hat{\beta}_{1}  $的估计结果是从最小二乘原理得到的, 故称为最小二乘估计量(least-squares estimators)。

记$  \left\{\begin{array}{c}\bar{X}=\frac{1}{n} \sum X_{i} \\ \bar{Y}=\frac{1}{n} \Sigma Y_{i} \\ x_{i}=X_{i}-\bar{X} \\ y_{i}=Y_{i}-\bar{Y}\end{array}\right. 
$

则参数估计量可以写成:
$$
\left\{\begin{array}{l}
\hat{\beta}_{1}=\frac{\sum x_{i} y_{i}}{\sum x_{i}^{2}} \\
\hat{\beta}_{0}=\bar{Y}-\hat{\beta}_{1} \bar{X}
\end{array}\right.

$$
记$ e_{i}=Y_{i}-\hat{Y}_{i}  $为第$  \mathrm{i} $个样本观测点的残差, 即被解释变量的估 计值与观测值之差, 则随机误差项方差的估计量为:
$$
\hat{\sigma}_{\mu}^{2}=\frac{\sum e_{i}^{2}}{n-2}
$$

##### 1.10.1.3 检验、预测与控制

###### 回归方程的显著性检验

对回归方程$  Y=\beta_{0}+\beta_{1} x  $的显著性检验, 归结为对假设
$$
H_{0}: \beta_{1}=0 ; H_{1}: \beta_{1} \neq 0
$$
进行检验:

假设$  H_{0}: \beta_{1}=0  $被拒绝, 则回归显著, 认为$  y  $与$  x  $存在线性关 系, 所求的线性回归方程有意义; 否则回归不显著,$  y  $与$  x  $的关系 不能用一元线性回归模型来描述, 所得的回归方程也无意义.
（1）$F  $检验法

当$  H_{0}  $成立时, 
$$
 F=\frac{U}{Q_{e} /(n-2)} \sim F(1, n-2) 
$$
其中
$$
  U=\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}  (回归平方和)
$$
故$  F>F_{1-\alpha}(1, n-2)$ , 拒绝$  H_{0} $, 否则就接受$  H_{0}$ .



（2）$t  $检验法

$H_{0}  $成立时,
$$
  T=\frac{\sqrt{L_{x x}} \hat{\beta}_{1}}{\hat{\sigma}_{e}} \sim \mathrm{t}(\mathrm{n}-2) 
$$
故$  |T|>t_{1-\frac{\alpha}{2}}(n-2)$ , 拒绝$  H_{0} $, 否则就接受$  H_{0} $.
其中
$$
L_{x x}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2}
$$


（3）$ r  $检验法
$$
  r=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2} \sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}}} 
$$
当$  |r|>r_{1-\alpha}  $时, 拒绝$  H_{0} $ ；否则就接受$  H_{0}$ .
其中
$$
  r_{1-\alpha}=\sqrt{\frac{1}{1+(n-2) / F_{1-\alpha}(1, n-2)}} 
$$


###### 回归系数的置信区间

$\beta_{0}  $和$  \beta_{1}  $置信水平为$  1-\alpha  $的置信区间分别为
$$
\left[\hat{\beta}_{0}-t_{1-\frac{\alpha}{2}}(n-2) \hat{\sigma}_{e} \sqrt{\frac{1}{n}+\frac{\bar{x}^{2}}{L_{x x}}}, \hat{\beta}_{0}+t_{1-\frac{\alpha}{2}}(n-2) \hat{\sigma}_{e} \sqrt{\frac{1}{n}+\frac{\bar{x}^{2}}{L_{x x}}}\right]
$$
和
$$
  \left[\hat{\beta}_{1}-t_{1-\frac{\alpha}{2}}(n-2) \hat{\sigma}_{e} / \sqrt{L_{x x}}, \hat{\beta}_{1}+t_{1-\frac{\alpha}{2}}(n-2) \hat{\sigma}_{e} / \sqrt{L_{x x}}\right] 
$$
$\sigma^{2}  $的置信水平为$ 1-  \alpha  $的置信区间为
$$
\left[\frac{Q_{e}}{\chi_{1-\frac{\alpha}{2}}^{2}(n-2)}, \frac{Q_{e}}{\chi_{\frac{\alpha}{2}}^{2}(n-2)}\right]
$$

###### 检测与控制

（1）预测

用$  y_{0}  $的回归值$  \hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{0}  $作为$  y_{0}  $的预测值.
$ y_{0}  $的置信水平为$  1-\alpha  $的预测区间为$\left[\hat{y}_{0}-\delta\left(x_{0}\right), \hat{y}_{0}+\delta\left(x_{0}\right)\right]$

其中
$$
  \delta\left(x_{0}\right)=\hat{\sigma}_{e} t_{1-\frac{\alpha}{2}}(n-2) \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{L_{x x}}} 
$$

特别, 当$  \mathrm{n}  $很大且$  \mathrm{x}_{0}  $在$  \bar{x}  $附近取值时,$  \mathrm{y}  $的置信水平为$  1-\alpha $的预测区间近似为:
$$
\left[\hat{y}-\hat{\delta}_{e} u_{1-\frac{\alpha}{2}}, \hat{y}+\hat{\delta}_{e} u_{1-\frac{\alpha}{2}}\right]
$$


（2）控制

要求：$y=\beta_0+\beta_1 x+\varepsilon$的值以$1-\alpha$的概率落在指定区间$({y}',{y}'')$

只要控制$x$满足以下两个等式
$$
\hat{y}-\delta(x)\ge{y}',\hat{y}+\delta(x)\le {y}''
$$
要求${y}''-{y}'\ge 2\delta(x)$。若$\hat y-\delta(x)={y}',\hat y-\delta(x)={y}''$分别有解${x}'$和${x}''$，即$\hat y-\delta({x}')={y}',\hat y-\delta({x}'')={y}''$

则$({x}',{x}'')$就是所求的$x$的控制区间

##### 1.10.1.4 可线性化的一元非线性回归（曲线回归）

配曲线的一般方法：

先对两个变量$x$和$y$作$n$次试验观察得$(x_i,y_i),i=1,2,..,n$画出散点图，根据散点图确定须配曲线的类型，然后由$n$对试验数据确定每一类曲线的未知参数$a$和$b$。采用的方法是通过变量代换把非线性回归化为线性回归，即采用非线性回归线性的方法

通常选择的六类曲线如下：

(1) 双曲线 $\frac{1}{y}=a+\frac{b}{x}$

(2) 幂函数曲线 $y=a x^b$, 其中 $x>0, a>0$

(3) 指数曲线 $y=a \mathrm{e}^{b x}$ 其中终数 $a>0$.

(4) 倒指数曲线 $y=a \mathrm{e}^{b / x}$ 其中 $a>0$,

(5) 对数曲线 $y=a+b \log x, x>0$

(6) $\mathbf{S}$ 型曲线 $y=\frac{1}{a+b \mathrm{e}^{-x}}$

#### 1.10.2 多元线性回归

##### 1.10.2.1 数学模型

一般称
$$
\left\{\begin{array}{}

Y=X\beta+\varepsilon \\
E(\varepsilon)=0,COV(\varepsilon,\varepsilon)=\sigma^2I_n
\end{array}
\right.
$$

$$
Y=\left[\begin{array}{c}

y_1 \\

y_2 \\

\vdots \\

y_n

\end{array}\right], X=\left[\begin{array}{ccccc}

1 & x_{11} & x_{12} & \ldots & x_{1k} \\

1 & x_{21} & x_{22} & \ldots & x_{2k} \\

\vdots & \vdots & \vdots & \ddots & \vdots \\

1 & x_{n 1} & x_{n 2} & \ldots & x_{n k}

\end{array}\right], \quad \beta=\left[\begin{array}{c}

\beta_0 \\

\beta_1 \\

\vdots \\

\beta_k

\end{array}\right], \quad \varepsilon=\left[\begin{array}{c}

\varepsilon_1 \\

\varepsilon_2 \\

\vdots \\

\varepsilon_n

\end{array}\right]
$$
$y=\beta_0+\beta_1 x_1+\ldots+\beta_k x_k$ 称为回归平面方程

线性模型$(Y,X\beta,\sigma^2I_n)$考虑的主要问题是：
（1）用试验值(样本值)对未知参数$\beta$和$\sigma^2$作点估计和假设检验，从而建立$y$与$x_1,x_2,...,x_k$之间的数量关系:

（2）在$x_1=x_{01},x_2=x_{02},...,x_k=x_{0k}$处对$y$的值作预测与控制，即对$y$作区间估计

##### 1.10.2.2 模型参数估计

###### 对 $\beta$ 和 $\sigma^2$ 作估计

用最小二乘法求 $\beta_0, \ldots, \beta_k$ 的估计量；作离差平方和
$$
Q=\sum_{i=1}^{n}\left(y_i-\beta_0-\beta_1 x_{i1}-\ldots-\beta_k x_{ik}\right)^2
$$
选择$\beta_0,...,\beta_k$使$Q$达到最小

解得估计值$\hat \beta=(X^TX)^{-1}(X^TX)$

得到的$\hat\beta_i$代入回归平面方程得：
$$
y=\hat\beta_0+\hat\beta_1x_1+\ldots+\hat\beta_kx_k
$$
称为经验回归平面方程,$\hat\beta_i$称为经验回归系数

###### 多项式回归

设变量$x,Y$的回归模型为
$$
Y=\beta_0+\beta_1 x+\beta_2 x^2+\cdots+\beta_p x^p+\varepsilon
$$
其中$p$为已知的，$\beta_i(i=1,2,...,p)$是未知参数，$\varepsilon$服从正态分布$N(0,\sigma^2)$
$$
Y=\beta_0+\beta_1 x+\beta_2 x^2+\ldots+\beta_k x^k
$$
称为回归多项式。上面的回归模型称为多项式回归

令$x_i=x^i,i=1,2,...,k$多项式回归模型变成多元线性回归模型

##### 1.10.2.3 多元线性回归中的检测与预测

###### 线性模型的回归系数检验

假设$H_0: \beta_0=\beta_1=\cdots=\beta_2=0$

（1）$F  $检验法
当$  H_{0}  $成立时,
$$
 F=\frac{U / k}{Q_e /(n-k-1)} \sim F(k, n-k-1) 
$$
如果$  F>F_{1-\alpha} (k, n-k-1) , $则拒绝$  H_{0}$ , 认为$  y  $与$  x_{1}, \cdots, x_{k}  $之间显著地有线性关系: 否则就接受$  H_{0} $, 认为$  y  $与$  x_{1}, \cdots, x_{k}  $之间线性关系不显著.
其中
$$
  U=\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}  (回旧平方和)  \\
  
  Q_{e}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2} (残差平方和)
$$
（2）$  r $检验法
定义$  R=\sqrt{\frac{U}{L_{y}}}=\sqrt{\frac{U}{U+Q_{e}}}  $为$  y  $与$  x_{l}, x_{2}, \ldots, x_{k}  $的多元相关系数或复相关系数.
由于$  F=\frac{n-k-1}{k} \frac{R^{2}}{1-R^{2}} , $故用$  F  $和用$  R  $捡验是等效的.

###### 预测

(1) 点预测
求出回归方程
$$
 \hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\cdots+\hat{\beta}_{k} x_{k} , 
$$
 对于给定自变量的值$  x_{1}^{*}, \cdots, x_{k}^{*} , $用
$$
\hat{y}^{*}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}^{*}+\cdots+\hat{\beta}_{k} x_{k}{ }^{*}  
$$
来预测
$$
  y^{*}=\beta_{0}+\beta_{1} x_{1}{ }^{*}+\cdots+\beta_{k} x_{k}{ }^{*}+\varepsilon .
$$
称$  \hat{y}^{*}  $为$  y^{*}  $的点预测.
(2) 区间预测
$ y  $的$  1-\alpha  $的预测 (置信) 区间为$  \left(\hat{y}_{1}, \hat{y}_{2}\right) $, 其中
$$
 \left\{\begin{array}{l}\hat{y}_{1}=\hat{y}-\hat{\sigma}_{e} \sqrt{1+\sum_{i=0}^{k} \sum_{j=0}^{k} c_{i j} x_{i} x_{j} t_{1-\alpha / 2}}(n-k-1) \\ \hat{y}_{2}=\hat{y}+\hat{\sigma}_{e} \sqrt{1+\sum_{i=0}^{k} \sum_{j=0}^{k} c_{i j} x_{i} x_{j} t_{1-\alpha / 2}}(n-k-1)\end{array}\right. \\ \hat{\sigma}_{e}=\sqrt{\frac{Q_{e}}{n-k-1}}\\  C=L^{-1}=\left(c_{i j}\right),\\L=X^{\mathrm{T}} X 
$$

##### 1.10.2.4 逐步回归分析

选择“最优”的回归方程有以下几种方法:

+ 从所有可能的因子(变量) 组合的回归方程中选择最优者
+ 从包含全部变量的回归方程中逐次剔除不显著因子:
+ 从一个变量开始，把变量逐个引入方程;
+ "有进有出"的**逐步回归分析**

逐步回归分析法的思想

+ 从一个自变量开始，视自变量对$Y$作用的显著程度，从大到小地依次逐个引入回归方程
+ 当引入的自变量由于后面变量的引入而变得不显著时，要将其剔除掉.
+ 引入一个自变量或从回归方程中剔除一个自变量，为逐步回归的一步.
+ 对于每一步都要进行$Y$值检验，以确保每次引入新的显薯性变量前回归方程中只包含对$Y$作用显著的变量.
+ 这个过程反复进行，直至既无不显著的变量从回归方程中剔除，又无显著变量可引入回归方程时为止.